services:
  # Qdrant Vector Database for RAG
  qdrant:
    image: qdrant/qdrant:latest
    container_name: llama-nexus-qdrant
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__HTTP_PORT=6333
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # Redis for event-driven architecture and caching
  redis:
    image: redis:7-alpine
    container_name: llama-nexus-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Backend Management API - Controls LlamaCPP
  backend-api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: llamacpp-backend
    ports:
      - "8700:8700"    # Management API port
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    environment:
      # Default LlamaCPP configuration - Qwen3-VL-4B-Thinking
      - MODEL_NAME=Qwen_Qwen3-VL-4B-Thinking
      - MODEL_VARIANT=Q4_K_M
      - MODEL_REPO=bartowski/Qwen_Qwen3-VL-4B-Thinking-GGUF
      - MMPROJ_FILE=mmproj-Qwen_Qwen3-VL-4B-Thinking-f16.gguf
      # Host path for chat templates so backend can mount it into the managed container
      - TEMPLATES_HOST_DIR=/home/alec/git/llama-nexus/chat-templates
      - CONTEXT_SIZE=40960
      - GPU_LAYERS=999
      - TEMPERATURE=0.7
      - TOP_P=0.8
      - TOP_K=20
      - REPEAT_PENALTY=1.05
      - BATCH_SIZE=2048
      - N_CPU_MOE=0
      - API_KEY=placeholder-api-key
      - TEMPLATE_PATH=/home/llamacpp/chat-template.jinja
      # Enable Docker mode and set container name
      - USE_DOCKER=true
      - RUNNING_IN_DOCKER=true
      - LLAMACPP_CONTAINER_NAME=llamacpp-api
      # Pass through HuggingFace token from host environment
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # Enable colored logging
      - FORCE_COLOR=1
      - TERM=xterm-256color
      # Redis for event bus and caching
      - REDIS_URL=redis://redis:6379/0
      # Qdrant for RAG vector storage
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      # GraphRAG external service (Neo4j + GLiNER)
      - GRAPHRAG_URL=http://graphrag-api-1:8000
      - GRAPHRAG_ENABLED=true
      # Database paths for persistent storage
      - TOKEN_DB_PATH=/data/token_usage.db
      - CONVERSATION_DB_PATH=/data/conversations.db
      - RAG_DB_PATH=/data/rag
    volumes:
      - backend_data:/data
      - gpt_oss_models:/home/llamacpp/models
      - ./chat-templates:/home/llamacpp/templates:ro
      # Mount llamacpp binaries
      - llamacpp_bin:/usr/local/bin:ro
      # Mount Docker socket to enable Docker operations
      - /var/run/docker.sock:/var/run/docker.sock
      # Mount entire project directory for rebuild operations (read-only except Dockerfile)
      - .:/home/alec/git/llama-nexus:ro
      # Mount Dockerfile separately as read-write for version management
      - ./Dockerfile:/home/alec/git/llama-nexus/Dockerfile
      # Conversation storage
      -       conversation_data:/data/conversations
    shm_size: '16gb'
    depends_on:
      llamacpp-builder:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    restart: unless-stopped

  # Build container for LlamaCPP binaries
  llamacpp-builder:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder
    container_name: llamacpp-builder
    volumes:
      - llamacpp_bin:/output
    command: |
      bash -c "
        cp /build/llama.cpp/build/bin/llama-server /output/
        cp /build/llama.cpp/build/bin/llama-cli /output/
        chmod +x /output/*
        echo 'LlamaCPP binaries copied'
      "

  # Llama.cpp API Server - Direct access
  llamacpp-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llamacpp-api
    hostname: llamacpp
    ports:
      - "8600:8080"    # API server port
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    volumes:
      - gpt_oss_models:/home/llamacpp/models
      - ./chat-templates:/home/llamacpp/templates:ro
    environment:
      - TEMPLATE_DIR=/home/llamacpp/templates
      - CHAT_TEMPLATE=builtin-gpt-oss
      # CUDA optimizations - use '0' for single GPU or '0,1' for multiple
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-0}
      # Model configuration - Qwen3-VL-4B-Thinking (Vision-Language with Reasoning)
      - MODEL_NAME=Qwen_Qwen3-VL-4B-Thinking
      - MODEL_VARIANT=Q4_K_M
      - MODEL_REPO=bartowski/Qwen_Qwen3-VL-4B-Thinking-GGUF
      - MMPROJ_FILE=mmproj-Qwen_Qwen3-VL-4B-Thinking-f16.gguf
      - CONTEXT_SIZE=40960
      - GPU_LAYERS=999
      - TEMPERATURE=0.6
      - TOP_P=0.95
      - TOP_K=20
      - API_KEY=placeholder-api-key
      - BATCH_SIZE=2048
      - N_CPU_MOE=0
      # HuggingFace token for downloads
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

  # Management Frontend - React Web Interface
  llamacpp-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: llamacpp-frontend
    hostname: frontend
    ports:
      - "3002:80"    # Frontend web interface
    depends_on:
      - backend-api
      # llamacpp-api dependency removed - GPU may not always be available
    environment:
      - VITE_API_BASE_URL=
      - VITE_BACKEND_URL=
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # vLLM API Server - Native MXFP4 GPT-OSS-20B
  vllm-api:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: vllm-api
    profiles: 
      - vllm
    hostname: vllm
    ports:
      - "8601:8080"    # vLLM API server port (different from llamacpp)
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    environment:
      - MODEL_NAME=openai/gpt-oss-20b
      - HOST=0.0.0.0
      - PORT=8080
      - API_KEY=placeholder-api-key
      - MAX_MODEL_LEN=131072
      - GPU_MEMORY_UTILIZATION=0.8
      - TENSOR_PARALLEL_SIZE=1
      - DTYPE=auto
      - TRUST_REMOTE_CODE=true
      - ENABLE_CHUNKED_PREFILL=true
      - MAX_NUM_BATCHED_TOKENS=8192
      - MAX_NUM_SEQS=256
      - SERVED_MODEL_NAME=gpt-oss-20b
      # HuggingFace token for downloads
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # CUDA optimizations - use '0' for single GPU or '0,1' for multiple
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-0}
    volumes:
      - vllm_models:/home/vllm/models
      - vllm_cache:/home/vllm/.cache
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 600s  # vLLM takes longer to start due to model download

  # Llama.cpp Embedding Server - Optional RAG Embedding Service
  llamacpp-embed:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llamacpp-embed
    profiles:
      - embed
    hostname: llamacpp-embed
    ports:
      - "8602:8080"    # Embedding server port
    shm_size: '8gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    volumes:
      - gpt_oss_models:/home/llamacpp/models
      - ./start-embed.sh:/start-embed.sh:ro
    environment:
      # Embedding model configuration - Nomic Embed v1.5
      - MODEL_NAME=nomic-embed-text-v1.5
      - MODEL_VARIANT=Q8_0
      - CONTEXT_SIZE=2048
      - GPU_LAYERS=0
      - HOST=0.0.0.0
      - PORT=8080
      - API_KEY=llamacpp-embed
      - THREADS=8
      - BATCH_SIZE=512
      - UBATCH_SIZE=512
      - POOLING_TYPE=mean
      # HuggingFace token for downloads
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # CUDA device - keep low GPU layers to avoid memory conflicts
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    entrypoint: ["/bin/bash", "/start-embed.sh"]

volumes:
  gpt_oss_models:
    driver: local
  llamacpp_bin:
    driver: local
  vllm_models:
    driver: local
  vllm_cache:
    driver: local
  conversation_data:
    driver: local
  redis_data:
    driver: local
  backend_data:
    driver: local
  qdrant_data:
    driver: local
