services:
  # Backend Management API - Controls LlamaCPP
  backend-api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: llamacpp-backend
    ports:
      - "8700:8700"    # Management API port
    environment:
      # Default LlamaCPP configuration - Qwen3-VL-4B-Thinking
      - MODEL_NAME=Qwen_Qwen3-VL-4B-Thinking
      - MODEL_VARIANT=Q4_K_M
      - MODEL_REPO=bartowski/Qwen_Qwen3-VL-4B-Thinking-GGUF
      - MMPROJ_FILE=mmproj-Qwen_Qwen3-VL-4B-Thinking-f16.gguf
      # Host path for chat templates so backend can mount it into the managed container
      - TEMPLATES_HOST_DIR=/home/alec/git/llama-nexus/chat-templates
      - CONTEXT_SIZE=40960
      - GPU_LAYERS=999
      - TEMPERATURE=0.7
      - TOP_P=0.8
      - TOP_K=20
      - REPEAT_PENALTY=1.05
      - BATCH_SIZE=2048
      - N_CPU_MOE=0
      - API_KEY=placeholder-api-key
      - TEMPLATE_PATH=/home/llamacpp/chat-template.jinja
      # Enable Docker mode and set container name
      - USE_DOCKER=true
      - LLAMACPP_CONTAINER_NAME=llamacpp-api
      # Pass through HuggingFace token from host environment
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # Enable colored logging
      - FORCE_COLOR=1
      - TERM=xterm-256color
    volumes:
      - gpt_oss_models:/home/llamacpp/models
      - ./chat-templates:/home/llamacpp/templates:ro
      # Mount llamacpp binaries
      - llamacpp_bin:/usr/local/bin:ro
      # Mount Docker socket to enable Docker operations
      - /var/run/docker.sock:/var/run/docker.sock
      # Mount entire project directory for rebuild operations (read-only except Dockerfile)
      - .:/home/alec/git/llama-nexus:ro
      # Mount Dockerfile separately as read-write for version management
      - ./Dockerfile:/home/alec/git/llama-nexus/Dockerfile
    runtime: nvidia
    shm_size: '16gb'
    depends_on:
      llamacpp-builder:
        condition: service_completed_successfully
    restart: unless-stopped

  # Build container for LlamaCPP binaries
  llamacpp-builder:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder
    container_name: llamacpp-builder
    volumes:
      - llamacpp_bin:/output
    command: |
      bash -c "
        cp /build/llama.cpp/build/bin/llama-server /output/
        cp /build/llama.cpp/build/bin/llama-cli /output/
        chmod +x /output/*
        echo 'LlamaCPP binaries copied'
      "

  # Llama.cpp API Server - Direct access
  llamacpp-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llamacpp-api
    hostname: llamacpp
    ports:
      - "8600:8080"    # API server port
    runtime: nvidia
    shm_size: '16gb'
    volumes:
      - gpt_oss_models:/home/llamacpp/models
      - ./chat-templates:/home/llamacpp/templates:ro
    environment:
      - TEMPLATE_DIR=/home/llamacpp/templates
      - CHAT_TEMPLATE=builtin-gpt-oss
      # CUDA optimizations
      - CUDA_VISIBLE_DEVICES=0,1
      - NVIDIA_VISIBLE_DEVICES=0,1
      # Model configuration - Qwen3-VL-4B-Thinking (Vision-Language with Reasoning)
      - MODEL_NAME=Qwen_Qwen3-VL-4B-Thinking
      - MODEL_VARIANT=Q4_K_M
      - MODEL_REPO=bartowski/Qwen_Qwen3-VL-4B-Thinking-GGUF
      - MMPROJ_FILE=mmproj-Qwen_Qwen3-VL-4B-Thinking-f16.gguf
      - CONTEXT_SIZE=40960
      - GPU_LAYERS=999
      - TEMPERATURE=0.6
      - TOP_P=0.95
      - TOP_K=20
      - API_KEY=placeholder-api-key
      - BATCH_SIZE=2048
      - N_CPU_MOE=0
      # HuggingFace token for downloads
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

  # Management Frontend - React Web Interface
  llamacpp-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: llamacpp-frontend
    hostname: frontend
    ports:
      - "3002:80"    # Frontend web interface
    depends_on:
      - backend-api
      - llamacpp-api
    environment:
      - VITE_API_BASE_URL=http://192.168.1.77:8600
      - VITE_BACKEND_URL=http://192.168.1.77:8700
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # vLLM API Server - Native MXFP4 GPT-OSS-20B
  vllm-api:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: vllm-api
    profiles: 
      - vllm
    hostname: vllm
    ports:
      - "8601:8080"    # vLLM API server port (different from llamacpp)
    runtime: nvidia
    shm_size: '16gb'
    environment:
      - MODEL_NAME=openai/gpt-oss-20b
      - HOST=0.0.0.0
      - PORT=8080
      - API_KEY=placeholder-api-key
      - MAX_MODEL_LEN=131072
      - GPU_MEMORY_UTILIZATION=0.8
      - TENSOR_PARALLEL_SIZE=1
      - DTYPE=auto
      - TRUST_REMOTE_CODE=true
      - ENABLE_CHUNKED_PREFILL=true
      - MAX_NUM_BATCHED_TOKENS=8192
      - MAX_NUM_SEQS=256
      - SERVED_MODEL_NAME=gpt-oss-20b
      # HuggingFace token for downloads
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # CUDA optimizations
      - CUDA_VISIBLE_DEVICES=0,1
      - NVIDIA_VISIBLE_DEVICES=0,1
    volumes:
      - vllm_models:/home/vllm/models
      - vllm_cache:/home/vllm/.cache
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 600s  # vLLM takes longer to start due to model download

volumes:
  gpt_oss_models:
    driver: local
  llamacpp_bin:
    driver: local
  vllm_models:
    driver: local
  vllm_cache:
    driver: local