services:
  # Qdrant Vector Database for RAG
  qdrant:
    image: qdrant/qdrant:latest
    container_name: llama-nexus-qdrant
    ports:
      - "6333:6333" # REST API
      - "6334:6334" # gRPC API
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__HTTP_PORT=6333
    healthcheck:
      test: [ "CMD-SHELL", "timeout 5 bash -c '</dev/tcp/localhost/6333' || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # Redis for event-driven architecture and caching
  redis:
    image: redis:7-alpine
    container_name: llama-nexus-redis
    ports:
      - "6389:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Backend Management API - Controls LlamaCPP
  backend-api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: llamacpp-backend
    ports:
      - "8700:8700" # Management API port
    environment:
      # Default LlamaCPP configuration - Qwen3-VL-4B-Thinking
      - MODEL_NAME=Qwen_Qwen3-VL-4B-Thinking
      - MODEL_VARIANT=Q4_K_M
      - MODEL_REPO=bartowski/Qwen_Qwen3-VL-4B-Thinking-GGUF
      # MMPROJ_FILE should be set via UI when deploying multimodal models
      # - MMPROJ_FILE=mmproj-Qwen_Qwen3-VL-4B-Thinking-f16.gguf
      # Host path for chat templates so backend can mount it into the managed container
      - TEMPLATES_HOST_DIR=/home/alec/git/llama-nexus/chat-templates
      - CONTEXT_SIZE=40960
      - GPU_LAYERS=999
      - TEMPERATURE=0.7
      - TOP_P=0.8
      - TOP_K=20
      - REPEAT_PENALTY=1.05
      - BATCH_SIZE=2048
      - N_CPU_MOE=0
      - API_KEY=placeholder-api-key
      - TEMPLATE_PATH=/home/llamacpp/chat-template.jinja
      # Enable Docker mode and set container name
      - USE_DOCKER=true
      - RUNNING_IN_DOCKER=true
      - LLAMACPP_CONTAINER_NAME=llamacpp-api
      # Pass through HuggingFace token from host environment
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # Enable colored logging
      - FORCE_COLOR=1
      - TERM=xterm-256color
      # Redis for event bus and caching
      - REDIS_URL=redis://redis:6379/0
      # Qdrant for RAG vector storage
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      # Fine-tuning configuration
      - USE_DOCKER_TRAINING=true
      - TRAINING_IMAGE=llama-nexus-training:latest
      - HF_HOME=/root/.cache/huggingface
      - FINETUNE_DATASET_DIR=/data/finetune_datasets
      - FINETUNE_JOB_DIR=/data/finetune_jobs
      # GraphRAG integrated module (use with --profile graphrag)
      - GRAPHRAG_ENABLED=${GRAPHRAG_ENABLED:-false}
      - NEO4J_URI=${NEO4J_URI:-bolt://neo4j:7687}
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      # GraphRAG GPU services (opt-in, use respective profiles)
      - NER_API_URL=${NER_API_URL:-}
      - REL_API_URL=${REL_API_URL:-}
      - CODE_RAG_URL=${CODE_RAG_URL:-}
      # Database paths for persistent storage
      - TOKEN_DB_PATH=/data/token_usage.db
      - CONVERSATION_DB_PATH=/data/conversations.db
      - RAG_DB_PATH=/data/rag
      # Reddit Crawler Configuration
      - REDDIT_CRAWLER_ENABLED=true
      - REDDIT_CRAWLER_INTERVAL_HOURS=6
      - REDDIT_CRAWLER_MAX_PER_RUN=50
      - REDDIT_CRAWLER_SUBREDDITS=tifu,offmychest,AmItheAsshole,confession,relationship_advice,pettyrevenge,MaliciousCompliance
      # VLM (Vision-Language Model) Configuration for PDF visual description
      - VLM_ENDPOINT_URL=${VLM_ENDPOINT_URL:-http://llamacpp-api:8080/v1}
      - VLM_API_KEY=${VLM_API_KEY:-placeholder-api-key}
      - VLM_MODEL_NAME=${VLM_MODEL_NAME:-Qwen_Qwen3-VL-4B-Thinking}
      - VLM_ENABLED=${VLM_ENABLED:-true}
      - USE_DEPLOYED_EMBEDDINGS=true
      - DEFAULT_EMBEDDING_MODEL=nomic-embed-text-v1.5
    volumes:
      - backend_data:/data
      - gpt_oss_models:/home/llamacpp/models
      - ./chat-templates:/home/llamacpp/templates:ro
      # Mount llamacpp binaries
      - llamacpp_bin:/usr/local/bin:ro
      # Mount Docker socket to enable Docker operations
      - /var/run/docker.sock:/var/run/docker.sock
      # Mount entire project directory for rebuild operations (read-only except Dockerfile)
      - .:/home/alec/git/llama-nexus:ro
      # Mount Dockerfile separately as read-write for version management
      - ./Dockerfile:/home/alec/git/llama-nexus/Dockerfile
      # Conversation storage
      - conversation_data:/data/conversations
    shm_size: '16gb'
    # GPU access for local embedding models (sentence-transformers)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      llamacpp-builder:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    restart: unless-stopped

  # Build container for LlamaCPP binaries
  llamacpp-builder:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder
      args:
        SKIP_BUILD_FROM_SOURCE: "false"
        LLAMACPP_VERSION: "b4610"
    container_name: llamacpp-builder
    volumes:
      - llamacpp_bin:/output
    command: |
      bash -c "
        cp /build/llama.cpp/build/bin/llama-server /output/
        cp /build/llama.cpp/build/bin/llama-cli /output/
        chmod +x /output/*
        echo 'LlamaCPP binaries copied'
      "

  # Llama.cpp API Server - Direct access
  llamacpp-api:
    profiles:
      - extra
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llamacpp-api
    hostname: llamacpp
    ports:
      - "8600:8080" # API server port
    shm_size: '16gb'
    runtime: nvidia
    volumes:
      - gpt_oss_models:/home/llamacpp/models
      - ./chat-templates:/home/llamacpp/templates:ro
    environment:
      - TEMPLATE_DIR=/home/llamacpp/templates
      - CHAT_TEMPLATE=builtin-gpt-oss
      # CUDA optimizations - use '0' for single GPU or '0,1' for multiple
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-0}
      # Model configuration - Qwen3-VL-4B-Thinking (Vision-Language with Reasoning)
      - MODEL_NAME=Qwen_Qwen3-VL-4B-Thinking
      - MODEL_VARIANT=Q4_K_M
      - MODEL_REPO=bartowski/Qwen_Qwen3-VL-4B-Thinking-GGUF
      # MMPROJ_FILE should be set for VL models only
      # - MMPROJ_FILE=mmproj-Qwen_Qwen3-VL-4B-Thinking-f16.gguf
      - CONTEXT_SIZE=40960
      - GPU_LAYERS=999
      - TEMPERATURE=0.6
      - TOP_P=0.95
      - TOP_K=20
      - API_KEY=placeholder-api-key
      - BATCH_SIZE=2048
      - N_CPU_MOE=0
      # HuggingFace token for downloads
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

  # Management Frontend - React Web Interface
  llamacpp-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: llamacpp-frontend
    hostname: frontend
    ports:
      - "3002:80" # Frontend web interface
    depends_on:
      - backend-api
      # llamacpp-api dependency removed - GPU may not always be available
    environment:
      - VITE_API_BASE_URL=
      - VITE_BACKEND_URL=
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:80/health" ]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # vLLM API Server - Native MXFP4 GPT-OSS-20B
  vllm-api:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: vllm-api
    profiles:
      - vllm
    hostname: vllm
    ports:
      - "8601:8080" # vLLM API server port (different from llamacpp)
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ 'all' ]
              capabilities: [ gpu ]
    environment:
      - MODEL_NAME=openai/gpt-oss-20b
      - HOST=0.0.0.0
      - PORT=8080
      - API_KEY=placeholder-api-key
      - MAX_MODEL_LEN=131072
      - GPU_MEMORY_UTILIZATION=0.8
      - TENSOR_PARALLEL_SIZE=1
      - DTYPE=auto
      - TRUST_REMOTE_CODE=true
      - ENABLE_CHUNKED_PREFILL=true
      - MAX_NUM_BATCHED_TOKENS=8192
      - MAX_NUM_SEQS=256
      - SERVED_MODEL_NAME=gpt-oss-20b
      # HuggingFace token for downloads
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # CUDA optimizations - use '0' for single GPU or '0,1' for multiple
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-0}
    volumes:
      - vllm_models:/home/vllm/models
      - vllm_cache:/home/vllm/.cache
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 600s # vLLM takes longer to start due to model download

  # Llama.cpp Embedding Server - Optional RAG Embedding Service
  llamacpp-embed:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llamacpp-embed
    hostname: llamacpp-embed
    ports:
      - "8602:8080" # Embedding server port
    shm_size: '8gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Use count: 1 to respect CUDA_VISIBLE_DEVICES for GPU pinning
              capabilities: [ gpu ]
    volumes:
      - gpt_oss_models:/home/llamacpp/models
      - ./start-embed.sh:/start-embed.sh:ro
    environment:
      # Embedding model configuration - Nomic Embed v1.5
      - MODEL_NAME=nomic-embed-text-v1.5
      - MODEL_VARIANT=Q8_0
      - MODEL_REPO=nomic-ai/nomic-embed-text-v1.5-GGUF
      - MODEL_FILE=nomic-embed-text-v1.5.Q8_0.gguf
      - CONTEXT_SIZE=2048
      - GPU_LAYERS=999
      - HOST=0.0.0.0
      - PORT=8080
      - API_KEY=llamacpp-embed
      - THREADS=8
      - BATCH_SIZE=1024
      - UBATCH_SIZE=1024
      - POOLING_TYPE=mean
      - PARALLEL=8
      # HuggingFace token for downloads
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # GPU device pinning - NVIDIA_VISIBLE_DEVICES is respected by Docker runtime
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    entrypoint: [ "/bin/bash", "/start-embed.sh" ]

  training:
    build:
      context: ./backend
      dockerfile: Dockerfile.training
    container_name: llama-nexus-training
    # GPU reservations for QLoRA training (requires sm_70+ GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ 'all' ]
              capabilities: [ gpu ]
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - REDIS_URL=redis://redis:6379/0
      - FINETUNE_DATASET_DIR=/data/finetune_datasets
      - FINETUNE_JOB_DIR=/data/finetune_jobs
      - HF_HOME=/root/.cache/huggingface
      # Set to false to enable GPU training with QLoRA (requires compatible GPU)
      - FORCE_CPU_TRAINING=false
    volumes:
      # Share the backend data volume for job configs and datasets
      - backend_data:/data
      # HuggingFace model cache
      - huggingface_cache:/root/.cache/huggingface
      # Models directory for saving merged models
      - gpt_oss_models:/home/llamacpp/models
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped

  # Quantization Worker - Model Quantization Service
  quantization-worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.quantization
    container_name: quantization-worker
    profiles:
      - quantize
    hostname: quantization-worker
    shm_size: '16gb'
    runtime: nvidia
    volumes:
      - gpt_oss_models:/home/llamacpp/models
      - backend_data:/data
      - quantization_output:/app/quantization_output
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - REDIS_URL=redis://redis:6379/0
      - MODELS_DIR=/home/llamacpp/models
      - QUANTIZATION_OUTPUT_DIR=/app/quantization_output
      - QUANTIZATION_DB_PATH=/data/quantization.db
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-0}
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python3", "-c", "import redis; redis.from_url('redis://redis:6379/0').ping()" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # LM-Eval Worker - GPU-accelerated model evaluation
  lm-eval-worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.lm-eval
    container_name: lm-eval-worker
    profiles:
      - eval  # Opt-in: start with 'docker compose --profile eval up -d'
    hostname: lm-eval-worker
    shm_size: '16gb'
    volumes:
      - gpt_oss_models:/home/llamacpp/models:ro
      - backend_data:/data
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - REDIS_URL=redis://redis:6379/0
      - MODELS_DIR=/home/llamacpp/models
      - LM_EVAL_OUTPUT_DIR=/data/lm_eval_results
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-all}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - LLAMACPP_API_URL=http://llamacpp-api:8080
      - LM_EVAL_API_KEY=placeholder-api-key
      - OPENAI_API_KEY=placeholder-api-key
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python3", "-c", "import redis; redis.from_url('redis://redis:6379/0').ping()" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Training image builder (run with: docker compose build training-image)
  training-image:
    build:
      context: ./backend
      dockerfile: Dockerfile.training
    image: llama-nexus-training:latest
    profiles:
      - build-only # Only build, don't run as a service
    command: [ "echo", "Training image built successfully" ]

  # =============================================================
  # STREAMING STT: NVIDIA Nemotron Speech Streaming
  # Real-time speech-to-text with cache-aware streaming inference
  # Start with: docker compose --profile streaming-stt up -d streaming-stt
  # =============================================================
  streaming-stt:
    build:
      context: ./streaming-stt
      dockerfile: Dockerfile
    container_name: streaming-stt
    profiles:
      - streaming-stt # Opt-in: requires GPU, doesn't start by default
    ports:
      - "8609:8009"
    # NVIDIA NeMo Framework requires these settings for proper SHMEM allocation
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      # The NeMo model to use for streaming STT
      - MODEL_NAME=${STREAMING_STT_MODEL:-nvidia/nemotron-speech-streaming-en-0.6b}
      # Audio chunk size sent from frontend (ms). Keep at 80ms for optimal latency
      - CHUNK_SIZE_MS=${STREAMING_STT_CHUNK_MS:-80}
      # Audio sample rate (Hz). Model expects 16kHz
      - SAMPLE_RATE=${STREAMING_STT_SAMPLE_RATE:-16000}
      # RMS level to detect speech start. Lower = more sensitive to quiet speech
      - VAD_THRESHOLD=${STT_VAD_THRESHOLD:-0.005}
      # RMS level to detect silence. Lower = more tolerant of quiet pauses
      - VAD_SILENCE_THRESHOLD=${STT_VAD_SILENCE_THRESHOLD:-0.005}
      # Debounce time for speech/silence transitions
      - VAD_HYSTERESIS_MS=${STT_VAD_HYSTERESIS_MS:-100}
      # Silence duration (ms) before pending text is committed as a sentence
      - SENTENCE_END_SILENCE_MS=${STT_SENTENCE_END_SILENCE_MS:-800}
      # Minimum word count for text to be committed as a sentence
      - MIN_SENTENCE_WORDS=${STT_MIN_SENTENCE_WORDS:-2}
      # Audio buffer before model inference (~900ms optimal for NeMo)
      - MODEL_BUFFER_MS=${STT_MODEL_BUFFER_MS:-896}
      # Logging level (DEBUG, INFO, WARNING, ERROR)
      - LOG_LEVEL=${STT_LOG_LEVEL:-INFO}
      - NVIDIA_VISIBLE_DEVICES=all
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8009/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s # NeMo model loading takes time
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: unless-stopped

  # ========================================
  # GraphRAG: Neo4j Knowledge Graph (optional)
  # Start with: docker compose --profile graphrag up -d
  # ========================================
  neo4j:
    image: neo4j:5.11
    container_name: llama-nexus-neo4j
    profiles:
      - graphrag
    ports:
      - "7474:7474"  # HTTP Browser
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2g
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:7474 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # ========================================
  # GraphRAG NER Service (GLiNER entity extraction)
  # Opt-in: Start with docker compose --profile graphrag-ner up -d
  # ========================================
  graphrag-ner:
    build:
      context: ./services/ner_api
      dockerfile: Dockerfile
    container_name: llama-nexus-ner
    profiles:
      - graphrag-ner
    ports:
      - "8711:8001"
    environment:
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/huggingface/transformers
    volumes:
      - ner_cache:/app/.cache
      - ner_models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  # ========================================
  # GraphRAG Relationship Extraction Service
  # Opt-in: Start with docker compose --profile graphrag-rel up -d
  # ========================================
  graphrag-rel:
    build:
      context: ./services/rel_api
      dockerfile: Dockerfile
    container_name: llama-nexus-rel
    profiles:
      - graphrag-rel
    ports:
      - "8712:8002"
    environment:
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/huggingface/transformers
    volumes:
      - rel_cache:/app/.cache
      - rel_models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  # ========================================
  # GraphRAG Code-RAG Service (code detection/search)
  # Opt-in: Start with docker compose --profile graphrag-code up -d
  # ========================================
  graphrag-code:
    build:
      context: ./services/code_rag
      dockerfile: Dockerfile
    container_name: llama-nexus-code-rag
    profiles:
      - graphrag-code
    ports:
      - "8713:8000"
    environment:
      - HF_HOME=/app/.cache/huggingface
      - GRAPHRAG_API_URL=http://backend-api:8700
    volumes:
      - code_rag_cache:/app/.cache
      - code_rag_models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

volumes:
  gpt_oss_models:
    driver: local
  llamacpp_bin:
    driver: local
  finetune_data:
    driver: local
  vllm_models:
    driver: local
  vllm_cache:
    driver: local
  conversation_data:
    driver: local
  redis_data:
    driver: local
  backend_data:
    driver: local
  qdrant_data:
    driver: local
  quantization_output:
    driver: local
  huggingface_cache:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  # GraphRAG service volumes
  ner_cache:
    driver: local
  ner_models:
    driver: local
  rel_cache:
    driver: local
  rel_models:
    driver: local
  code_rag_cache:
    driver: local
  code_rag_models:
    driver: local
